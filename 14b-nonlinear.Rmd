# 非線形回帰モデル・分類モデル

重回帰分析では、アウトカムが量的変数である線形回帰モデルを主に考えました。ここでは、アウトカムが量的変数で説明変数に対して非線形の関係を持つ場合、もしくは質的変数である場合のモデルを考えます。



__パラメトリックな非線形モデル__

- 潜在変数$Z_i = \beta_0 + \beta_1 X_i + \varepsilon_i$ が何らかの閾値$\mu$を越える確率として考えることができ、$\varepsilon_i$の分布を$F$と書くと、$Y_i \in \{0,1\}$に対して、
  $$
  \mathbb{E}Y_{i}=F\left(\beta_{0}+\beta_{1}X_{i}\right)
  $$
  などとして書ける。

  - プロビット(probit) ... $F(\varepsilon_i) = \Phi(\varepsilon_i)$、正規分布を仮定
  - ロジット(logit) ... $F(\varepsilon_i) = \exp(\varepsilon_i)/[1 + \exp(\varepsilon_i)]$ 、ロジスティック分布を仮定

- 限界効果$\partial \mathbb{E}Y_{i}/ \partial X_i = \beta_i f\left(\beta_{0}+\beta_{1}X_{i}\right)$に着目して、係数を解釈する

- 被説明変数が離散的で2つ以上の値を取るとき、説明変数が一つ以上のときにも適用できる

- (確率のためには非線形モデルを用いることが多いが、線形モデルを使うこともある)



__多項式と縮小推定量__

- 経済学の実証研究で最もよく用いるフレクシブルな非線形モデルは、多項式(polynomial)モデルと交互作用(interaction effects)モデルである

  - 多項式モデル ... $Y_i = \beta_0 + \beta_1 X_{i} + \beta_2 X_{i}^2 +... + \beta_k X_{i}^k + \varepsilon_i$ 
  - 交互作用モデル ... $Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{1i}X_{2i} + \varepsilon_i$ 

- テイラー近似 ... 十分な数の項を用いれば、(限定的な変域において)スムーズな関数を任意に近似できる

- 変数の間の相関が強いとき(多項式の高次項など)、多重共線性によって推定値の分散がとても大きくなってしまう

- 縮小推定量 ... 目的関数に罰則項を加え、推定値にバイアスを含めることでバリアンスを減らすことができる
  $$
  \sum_i^N \varepsilon_i^2 + \lambda \sum_j^k[\alpha  \beta_j^2 + (1-\alpha) |\beta_j|]
  $$

  - $\alpha = 1$ ... リッジ回帰、$\alpha = 0$ ... LASSO回帰、$\alpha \in (0,1)$ ... elastic net回帰
  - 推定された係数にはバイアスがかかっているため、解釈には注意が必要

  

__決定木とランダム・フォレスト__

- 決定木 ... 説明変数に応じて条件分岐を繰り返すことで分類・回帰をするモデル
- 決定木の「葉ノード」が十分にあれば、任意の関数に完全に適合できる
- しかし、データについて「葉ノード」が多すぎるとき、過剰適合となってしまう
- ランダム・フォレスト ... ブートストラップ法で標本サンプルを複製し、それぞれのデータで決定木を推定し、統合する推定方法
  - バギング(bootstrap aggregating)とよばれるアンサンブル学習法(ensemble learning)
  - それぞれの決定木における低いバイアスを活かしつつ、バリアンスを下げられる



__人工ニューラルネットと深層学習__

- 特に画像、音声、テキストなどの非構造データを分析するためにとても有効な手法

- パーセプトロン ... 入力$X$に重み$w$をつけ、活性化関数$f()$を通じて出力$Y$を決定する関数である
  $$
  Y_i = f(w_0 + \sum_k w_k X_{ik})
  $$

  - 例: $f(u) = \max(u, 0)$、$f(u) = \exp(u)/(1+\exp(u))$ 

- 人工ニューラルネットとは、複数のパーセプトロンをつなぎあわせた「隠れ層」を持つ複合関数である
  $$
  Y_i = f(z_0 + \sum_j z_jf(w_{0j} + \sum_k w_{kj} X_{ik}))
  $$

- パーセプトロンが十分に多くあり、活性化関数が非線形であれば、(たとえ「隠れ層」が浅くとも) あらゆる関数を任意に近似できる (万能近似定理)

- 深層学習 ... 「隠れ層」が複数あり、ニューラルネットが深いとき、(比較的)少ないパラメータで複雑な関数を近似できる

  - 2012年に画像認識で世界一だったAlexNet ... $6 \times 10^8$ のパーセプトロンのつながり
  - 人間の脳 ... $10^{15}$の脳神経のつながり

- 重要な技術革新を経て、いくつもの障壁を乗り越えてきた

  1. 大量のデータ
  2. 逆伝播法と確率的勾配法
  3. 畳み込み層
  4. ドロップアウト法



---

Mikhail Belkin, Daniel Hsu, Siyuan Ma,  Soumik Mandal "Reconciling modern machine-learning practice and the classical bias–variance trade-off" 2019, PNAS

今泉允聡『深層学習の原理に迫る　数学の挑戦』岩波書店、2021年

ショーン・ジェリッシュ 『スマートマシンはこうして思考する』(How Smart Machines Think) 依田光江訳、みすず書房、2020年

梅津佑太、西井龍映、上田勇祐 『スパース回帰分析とパターン認識』講談社、2020年

西山慶彦・新谷元嗣・川口大司・奥井亮『計量経済学 Econometrics: Statistical Data Analysis for Empirical Economics』New Liberal Arts Selection、 有斐閣、2019年

立山秀利 『日経ソフトウエアのディープラーニングAIはどのように学習し、推論しているのか』2021. 日経BP.

Sendhil Mullainathan and Jann Spiess. 2017. ”Machine Learning: An Applied Econometric Approach." Journal of Economic Perspectives

Hal Varian. 2014. "Big Data: New Tricks for Econometrics." Journal of Economic Perspectives.

Stuart Geman, Elie Bienenstock, Rene Doursat "Neural Networks and the Bias/Variance Dilemma" 1992. Neural Computation.

Bradley Efron, Trevor Hastie 『大規模計算時代の統計推論―原理と発展― (Computer Age Statistical Inference: Algorithms, Evidence, and Data Science)』藤澤 洋徳・井手 剛監訳・井尻 善久・井手 剛・牛久 祥孝・梅津 佑太・大塚 琢馬・尾林 慶一・川野 秀一・田栗 正隆・竹内 孝・橋本 敦史・藤澤 洋徳・矢野 恵佑訳 2020. 共立出版

Yann LeCun 『ディープラーニング 学習する機械 -- ヤン・ルカン、人工知能を語る』2021. 松尾豊監訳/ 小川浩一訳. 講談社

富谷昭夫 『これならわかる機械学習入門』 2021. 講談社

岩沢宏和、平松雄司 『入門 Rによる予測モデリング――機械学習を用いたリスク管理のために』2019. 東京図書

岡野原大輔 『ディープラーニングを支える技術 ——「正解」を導くメカニズム[技術基礎] 』2022. 技術評論社

加藤公一、秋庭伸也、杉山阿聖、寺田学 『機械学習図鑑　見て試してわかる機械学習アルゴリズムの仕組み』2019. 翔泳社

